{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f8c243",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "```bash\n",
    "python -m src.train.train_traj_V3 --config configs/traisformer.yaml\n",
    "```\n",
    "\n",
    "## Evaluate model\n",
    "```bash\n",
    "python -m src.eval.eval_traj_V3   --split_dir data/map_reduced/test   --ckpt data/checkpoints/traj_traisformer.pt   --model traisformer   --horizon 24 --past_len 128 --pred_cut 80   --mmsi all   --out_dir data/figures/traisformer_all_80   --auto_extent --samples 16 --temperature 0.8 --top_k 30 --match_distance --verbose\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "python -m src.eval.eval_traj_V3   --split_dir data/map_reduced/test   --ckpt data/checkpoints/traj_traisformer.pt   --model traisformer   --horizon 24 --past_len 128 --pred_cut 80   --mmsi 205482000,209184000    --out_dir data/figures/traisformer_sub_80_test01   --auto_extent --samples 8 --temperature 0.8 --top_k 30 --match_distance --verbose\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m src.eval.eval_traj_V4   --split_dir data/map_reduced/test   --ckpt data/checkpoints/traj_traisformer.pt   --model traisformer   --horizon 24 --past_len 128 --pred_cut 80   --mmsi 205482000,209184000    --out_dir data/figures/traisformer_v4_test27   --auto_extent --samples 24 --temperature 1.05 --top_k 60 --match_distance --verbose\n",
    "```\n",
    "\n",
    "\n",
    "## TPTrans\n",
    "```bash\n",
    "python -m src.eval.eval_traj \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_tptrans.pt \\\n",
    "  --model tptrans \\\n",
    "  --horizon 12 \\\n",
    "  --past_len 64 \\\n",
    "  --pred_cut 80 \\\n",
    "  --min_points 60 \\\n",
    "  --mmsi 205482000,209184000 \\\n",
    "  --out_dir data/figures/val_subset_tptrans_cut80_test01 \\\n",
    "  --auto_extent \\\n",
    "  --match_distance \\\n",
    "  --verbose\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ebb57",
   "metadata": {},
   "source": [
    "5) How to run (with your folder layout)\n",
    "Train (TrAISformer)\n",
    "python -m src.train.train_traj_V3 --config configs/traisformer.yaml\n",
    "\n",
    "\n",
    "Uses data/map_reduced/train and .../val\n",
    "\n",
    "Saves data/checkpoints/traj_traisformer.pt with {state_dict, bins}\n",
    "\n",
    "Evaluate (one MMSI, one trip)\n",
    "python -m src.eval.eval_traj_V3 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_traisformer.pt \\\n",
    "  --model traisformer \\\n",
    "  --horizon 24 \\\n",
    "  --past_len 128 \\\n",
    "  --pred_cut 75 \\\n",
    "  --mmsi 210046000 \\\n",
    "  --trip_id 0 \\\n",
    "  --out_dir data/figures/traisformer_single \\\n",
    "  --auto_extent \\\n",
    "  --samples 8 --temperature 0.8 --top_k 30 --match_distance\n",
    "\n",
    "Evaluate (subset or all)\n",
    "# subset\n",
    "python -m src.eval.eval_traj_V3 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_traisformer.pt \\\n",
    "  --model traisformer \\\n",
    "  --horizon 24 --past_len 128 --pred_cut 90 \\\n",
    "  --mmsi 210046000,210174000 \\\n",
    "  --out_dir data/figures/traisformer_subset \\\n",
    "  --auto_extent --samples 8 --temperature 0.8 --top_k 30 --match_distance\n",
    "\n",
    "# all\n",
    "python -m src.eval.eval_traj_V3 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_traisformer.pt \\\n",
    "  --model traisformer \\\n",
    "  --horizon 24 --past_len 128 --pred_cut 80 \\\n",
    "  --mmsi all \\\n",
    "  --out_dir data/figures/traisformer_all_80 \\\n",
    "  --auto_extent --samples 8 --temperature 0.8 --top_k 30 --match_distance --verbose\n",
    "\n",
    "\n",
    "Outputs are exactly as before:\n",
    "\n",
    "per-MMSI subfolders with PNG + per-trip CSV + metrics_<MMSI>.csv\n",
    "\n",
    "global summary_trips.csv, summary_by_mmsi.csv, summary_overall.csv at --out_dir.\n",
    "\n",
    "Final notes / sanity checks\n",
    "\n",
    "If your pickles store degrees/knots already, remove the *sog_max and *360 multipliers in to_bins(...) and eval.\n",
    "\n",
    "Keep the same bins for train and eval (we store them in the checkpoint).\n",
    "\n",
    "For better long-tail behavior, raise window to 128 (or 96) and horizon to 24‚Äì48, train for 30‚Äì40 epochs, and use --samples 8‚Äì16 at eval (best-of-N like the paper).\n",
    "\n",
    "If you want, I can also add a runnable diff (patch format) for each file, but the blocks above are ready to paste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a7869",
   "metadata": {},
   "source": [
    "# TrAISformer\n",
    "python -m src.eval.eval_traj_V4 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_traisformer.pt \\\n",
    "  --model traisformer \\\n",
    "  --past_len 128 --pred_cut 80 \\\n",
    "  --mmsi 205482000,209184000 \\\n",
    "  --out_dir data/figures/traisformer_v4_test01 \\\n",
    "  --auto_extent --samples 8 --temperature 0.8 --top_k 30 \\\n",
    "  --match_distance \\\n",
    "  --lat_min 54 --lat_max 58 --lon_min 6 --lon_max 16 --sog_max 50 \\\n",
    "  --n_lat 256 --n_lon 512 --n_sog 50 --n_cog 72 \\\n",
    "  --t_d_model 512 --t_nhead 8 --t_enc_layers 8 --t_dropout 0.1 \\\n",
    "  --t_coarse_merge 3 --t_coarse_beta 0.2\n",
    "\n",
    "\n",
    "python -m src.eval.eval_traj_V4 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_traisformer.pt \\\n",
    "  --model traisformer \\\n",
    "  --horizon 24 --past_len 128 --pred_cut 80 \\\n",
    "  --mmsi 205482000,209184000 \\\n",
    "  --out_dir data/figures/traisformer_v4_test33 \\\n",
    "  --auto_extent \\\n",
    "  --samples 32 --temperature 1.1 --top_k 80 \\\n",
    "  --match_distance \\\n",
    "  --lat_min 54 --lat_max 58 --lon_min 6 --lon_max 16 --sog_max 50 \\\n",
    "  --n_lat 384 --n_lon 768 --n_sog 50 --n_cog 72 \\\n",
    "  --t_d_model 512 --t_nhead 8 --t_enc_layers 8 --t_dropout 0.1 \\\n",
    "  --t_coarse_merge 3 --t_coarse_beta 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TPTrans\n",
    "python -m src.eval.eval_traj_V3 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_tptrans.pt \\\n",
    "  --model tptrans \\\n",
    "  --horizon 12 --past_len 64 \\\n",
    "  --pred_cut 80 --mmsi 205482000,209184000 \\\n",
    "  --out_dir data/figures/tptrans_v3_test01 \\\n",
    "  --auto_extent\n",
    "\n",
    "\n",
    "python -m src.eval.eval_traj_V4 \\\n",
    "  --split_dir data/map_reduced/test \\\n",
    "  --ckpt data/checkpoints/traj_tptrans.pt \\\n",
    "  --model tptrans \\\n",
    "  --horizon 12 --past_len 64 \\\n",
    "  --pred_cut 80 --mmsi 205482000,209184000 \\\n",
    "  --out_dir data/figures/tptrans_v4_test03 \\\n",
    "  --auto_extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3072be",
   "metadata": {},
   "source": [
    "Totally‚Äîhere‚Äôs a clean ‚Äúcheat sheet‚Äù for every knob you‚Äôve been touching, what it does, and when to move it.\n",
    "\n",
    "# Data & grid (binning)\n",
    "\n",
    "* `--lat_min/--lat_max`, `--lon_min/--lon_max`\n",
    "  Geographic box (degrees) used to discretize lat/lon. Must cover your data. Too tight ‚Üí clipping; too wide ‚Üí coarser effective resolution.\n",
    "\n",
    "* `--n_lat`, `--n_lon`\n",
    "  Number of bins (grid cells). Higher = finer shoreline/channel detail (good for √òresund), but more classes to predict (harder).\n",
    "  Typical: `n_lat=256, n_lon=512` for regional runs.\n",
    "\n",
    "* `--sog_max`, `--n_sog`\n",
    "  Speed bins from 0..`sog_max` (kn) split into `n_sog`. If ships go faster than `sog_max`, they get clipped.\n",
    "\n",
    "* `--n_cog`\n",
    "  Course-over-ground bins over 0‚Äì360¬∞. e.g. 72 ‚Üí 5¬∞ per bin.\n",
    "\n",
    "# Model (TrAISformer) hyperparameters\n",
    "\n",
    "* `--t_d_model` (`d_model`)\n",
    "  Transformer width. Larger learns more, costs more VRAM.\n",
    "\n",
    "* `--t_nhead` (`nhead`)\n",
    "  Attention heads. Keep `d_model % nhead == 0`. More heads = more angular resolution in attention patterns.\n",
    "\n",
    "* `--t_enc_layers` (`num_layers`)\n",
    "  Decoder depth. More layers = more capacity; slower.\n",
    "\n",
    "* `--t_dropout` (`dropout`)\n",
    "  Regularization. Slightly higher improves robustness on small data.\n",
    "\n",
    "* `--t_coarse_merge` / `--t_coarse_beta`\n",
    "  Coarse-to-fine loss: we also compute CE on coarser bins (merging every `merge` cells) with weight `beta`. Helps stability for big `n_lat/n_lon`.\n",
    "\n",
    "# Decoding / sampling (how futures are generated)\n",
    "\n",
    "* `--samples`\n",
    "  Number of stochastic rollouts to try; the script keeps the best one (lowest ADE). More = better chance to find a good trajectory; slower.\n",
    "\n",
    "* `--temperature`\n",
    "  Softens/sharpens logits.\n",
    "  Lower (0.5‚Äì0.8): conservative, sticks to high-prob bins.\n",
    "  Higher (1.0‚Äì1.3): more exploration/turns.\n",
    "\n",
    "* `--top_k`\n",
    "  Keep only the top-k bins per attribute before sampling.\n",
    "  `k=0` (or omitted) = no truncation. Use small k (e.g., 20‚Äì50) to avoid weird tails.\n",
    "\n",
    "* **Joint lat/lon sampling (in code)**\n",
    "  In your current file we do a *local* joint choice with:\n",
    "\n",
    "  * `LAT_RAD`, `LON_RAD` ‚Äî half-window size (in bins) around the previous water cell. Wider lets you turn/move farther per step.\n",
    "  * `LAMBDA_CONT` ‚Äî continuity weight; higher hugs the current point; lower moves more.\n",
    "    Tip: `LAT_RAD=6‚Äì8`, `LON_RAD=12‚Äì16`, `LAMBDA_CONT=0.1‚Äì0.25` work well near coasts.\n",
    "\n",
    "> If you later switch to the **global joint sampler** I proposed, you‚Äôll also see:\n",
    ">\n",
    "> * `ALPHA_DIR` ‚Äî pushes along heading from COG/SOG (0.6‚Äì1.0).\n",
    "> * `BETA_TURN` ‚Äî penalizes sharp reversals (0.2‚Äì0.4).\n",
    "\n",
    "# Coastline / land constraint\n",
    "\n",
    "* **Water mask** (built in `build_water_mask.py`)\n",
    "\n",
    "  * Uses Natural Earth land polygons via Cartopy/Shapely.\n",
    "  * Make sure you installed deps and use resolution `\"10m\"` (sharper coastlines).\n",
    "  * The mask shape must match `(n_lat, n_lon)`.\n",
    "  * Quick sanity: print `water_fraction` (‚âà0.3‚Äì0.8 is healthy; ~1.0 means ‚Äúall water‚Äù ‚Üí bug).\n",
    "\n",
    "* **Hard land enforcement (in generation)**\n",
    "  We apply the mask *before* picking `(lat,lon)`. Land cells get `-inf` score ‚Üí impossible to select. That‚Äôs the strongest guarantee ‚Äúnever go on land‚Äù.\n",
    "\n",
    "# Evaluation / plotting\n",
    "\n",
    "* `--split_dir`\n",
    "  Folder with processed trips to evaluate.\n",
    "\n",
    "* `--mmsi`\n",
    "  Comma-separated MMSIs to filter.\n",
    "\n",
    "* `--past_len`\n",
    "  Number of past points given to the model.\n",
    "\n",
    "* `--pred_cut`\n",
    "  Percent of the trip kept as ‚Äúpast‚Äù. E.g., 80 ‚Üí predict the last 20%.\n",
    "\n",
    "* `--auto_extent`\n",
    "  Plot extents cropped to each trip. Good for zooming in.\n",
    "\n",
    "* `--out_dir`\n",
    "  Where to save PNGs + CSV summaries.\n",
    "\n",
    "* **Metrics** (printed on the plots)\n",
    "\n",
    "  * **ADE**: average distance error over all future steps (km).\n",
    "  * **FDE**: final-step distance error (km).\n",
    "  * **MAE** (here): average pointwise error (km). Lower is better.\n",
    "\n",
    "# Practical tuning playbook\n",
    "\n",
    "**If it ‚Äústicks‚Äù near the black dot**\n",
    "\n",
    "* Increase `LAT_RAD`, `LON_RAD`.\n",
    "* Decrease `LAMBDA_CONT`.\n",
    "* Raise `temperature` a bit; try `--samples 32` and keep `top_k` modest (e.g., 30‚Äì60).\n",
    "\n",
    "**If it overshoots or wanders**\n",
    "\n",
    "* Decrease window radii and/or increase `LAMBDA_CONT`.\n",
    "* Lower `temperature` and/or reduce `samples`.\n",
    "\n",
    "**If it tries to cross land**\n",
    "\n",
    "* Verify the mask: water_fraction < 0.99 and coastlines at `\"10m\"`.\n",
    "* Ensure masking happens **before** sampling (as in your file).\n",
    "* Keep `n_lat/n_lon` high enough so narrow channels exist on-grid.\n",
    "\n",
    "**If it goes straight east/west**\n",
    "\n",
    "* Usually logits are peaky in one axis; widen window and/or add heading prior (global sampler variant).\n",
    "* Check that lat/lon inputs are **denormalized to degrees** before binning (you already fixed this in eval).\n",
    "\n",
    "**If √òresund behavior is flaky**\n",
    "\n",
    "* `n_lat‚â•256`, `n_lon‚â•512`, `\"10m\"` coast, `LAT_RAD‚âà6‚Äì8`, `LON_RAD‚âà12‚Äì16`, `LAMBDA_CONT‚âà0.15‚Äì0.2`, `temperature‚âà0.8‚Äì1.0`, `top_k‚âà30‚Äì60`, `samples‚âà16‚Äì64`.\n",
    "\n",
    "# Quick meanings recap (one-liners)\n",
    "\n",
    "* **Grid size** (`n_lat/n_lon`): map resolution.\n",
    "* **Ranges** (`lat/lon_min/max`): map bounds.\n",
    "* **SOG/COG bins** (`n_sog/n_cog`, `sog_max`): kinematic discretization.\n",
    "* **d_model / nhead / layers / dropout**: transformer capacity.\n",
    "* **coarse_merge / coarse_beta**: extra coarse CE to stabilize learning.\n",
    "* **samples / temperature / top_k**: exploration vs. conservatism at decode.\n",
    "* **LAT_RAD / LON_RAD / LAMBDA_CONT**: how far you can step per tick and how strongly you prefer staying near the previous cell.\n",
    "* **(optional) ALPHA_DIR / BETA_TURN**: go-forward bias & anti-zigzag.\n",
    "* **water mask (10m)**: hard ‚Äúno land‚Äù gate.\n",
    "\n",
    "If you want, I can flip your file to the global joint sampler with heading/anti-zigzag (it tends to reduce those little loops) and set sensible defaults for √òresund.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ad43a",
   "metadata": {},
   "source": [
    "Perfect ‚Äî let‚Äôs go through your **TrAISformer** step by step, both in *architecture* and *theory*, so you understand exactly what each part does and how it fits together.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1. The Big Picture\n",
    "\n",
    "The **TrAISformer** is a **sequence prediction model** inspired by NLP Transformers ‚Äî but adapted to predict **ship trajectories**.\n",
    "It processes sequences of AIS data (latitude, longitude, SOG, COG) and predicts future positions.\n",
    "\n",
    "Conceptually, you can imagine the model as:\n",
    "\n",
    "> **Embedding (CNN-like feature extraction)** ‚Üí **Transformer (sequence modeling)** ‚Üí **Feedforward Heads (attribute prediction)**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2. Input Data Representation\n",
    "\n",
    "AIS data comes as time series:\n",
    "\n",
    "| Time step | Latitude | Longitude | SOG  | COG |\n",
    "| --------- | -------- | --------- | ---- | --- |\n",
    "| t‚ÇÅ        | 55.7     | 12.3      | 10.2 | 45¬∞ |\n",
    "| t‚ÇÇ        | 55.8     | 12.4      | 10.4 | 50¬∞ |\n",
    "| ...       | ...      | ...       | ...  | ... |\n",
    "\n",
    "But Transformers can‚Äôt process raw floats.\n",
    "So your model **discretizes** them into *bins* ‚Äî like pixels in a grid:\n",
    "\n",
    "* **lat** ‚Üí [0, n_lat)\n",
    "* **lon** ‚Üí [0, n_lon)\n",
    "* **SOG** ‚Üí [0, n_sog)\n",
    "* **COG** ‚Üí [0, n_cog)\n",
    "\n",
    "These indices are then used like *token IDs* in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## üî§ 3. Embedding Layer (Analogy: CNN feature extractor)\n",
    "\n",
    "Each discrete attribute (lat, lon, sog, cog) has its own **embedding matrix**:\n",
    "\n",
    "```python\n",
    "self.lat_emb = nn.Embedding(bins.n_lat, emb_lat)\n",
    "self.lon_emb = nn.Embedding(bins.n_lon, emb_lon)\n",
    "self.sog_emb = nn.Embedding(bins.n_sog, emb_sog)\n",
    "self.cog_emb = nn.Embedding(bins.n_cog, emb_cog)\n",
    "```\n",
    "\n",
    "Each embedding learns a *latent representation* ‚Äî a vector that captures relationships like:\n",
    "\n",
    "* Nearby lat/lon cells mean nearby physical positions.\n",
    "* Similar COG bins represent similar headings.\n",
    "\n",
    "Then, these four embeddings are concatenated into one vector:\n",
    "\n",
    "```\n",
    "e_t = [e_lat | e_lon | e_sog | e_cog]\n",
    "```\n",
    "\n",
    "This is like the **feature extraction stage** of a CNN ‚Äî compressing raw symbolic input into a continuous feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ 4. Input Projection and Positional Encoding\n",
    "\n",
    "You then map this concatenated embedding into the model‚Äôs ‚Äúworking space‚Äù:\n",
    "\n",
    "```python\n",
    "z = self.in_proj(e_t)\n",
    "```\n",
    "\n",
    "This linear projection acts like a **1√ó1 convolution** in CNNs ‚Äî aligning all attributes into a unified dimension `d_model`.\n",
    "\n",
    "Then, you add **sinusoidal positional encodings**:\n",
    "\n",
    "```python\n",
    "z = z + pos_encoding\n",
    "```\n",
    "\n",
    "These give the model a sense of **time order** (since Transformers themselves are permutation-invariant).\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ 5. Transformer Decoder (Sequence Model)\n",
    "\n",
    "This is the heart of the TrAISformer ‚Äî the **temporal modeling** part.\n",
    "\n",
    "### Structure:\n",
    "\n",
    "Each `nn.TransformerDecoderLayer` contains:\n",
    "\n",
    "1. **Self-Attention**:\n",
    "   Each time step looks at all previous ones to decide which past points are important for predicting the next.\n",
    "   [\n",
    "   \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "   ]\n",
    "   This is how it captures temporal dependencies, turning ‚Äúpast movement‚Äù into ‚Äúexpected motion‚Äù.\n",
    "\n",
    "2. **Cross-Attention** (to the past memory):\n",
    "   During training, the decoder can attend to both past positions (`memory=z_past`) and current partially predicted future steps.\n",
    "\n",
    "3. **Feedforward (FFN) sub-layer**:\n",
    "   Two linear layers with ReLU/gelu activation in between ‚Äî this is the *neural processing* inside each Transformer block:\n",
    "   [\n",
    "   \\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "   ]\n",
    "\n",
    "4. **Residual connections + LayerNorm** help stabilize training.\n",
    "\n",
    "With multiple stacked layers (8 by default), you get a **hierarchical representation of temporal patterns** ‚Äî like routes, maneuvers, or turning behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 6. Output Heads (Feedforward Classifiers)\n",
    "\n",
    "Once the Transformer has encoded the context, the model decodes it into predictions.\n",
    "\n",
    "Each attribute has its own **fully connected output head**:\n",
    "\n",
    "```python\n",
    "self.head_lat = nn.Linear(d_model, bins.n_lat)\n",
    "self.head_lon = nn.Linear(d_model, bins.n_lon)\n",
    "self.head_sog = nn.Linear(d_model, bins.n_sog)\n",
    "self.head_cog = nn.Linear(d_model, bins.n_cog)\n",
    "```\n",
    "\n",
    "That‚Äôs four separate **Feedforward Neural Networks (FFNNs)** ‚Äî each mapping the Transformer‚Äôs hidden vector into a probability distribution over bins.\n",
    "\n",
    "At every step, the model predicts:\n",
    "[\n",
    "P(\\text{lat}_t),; P(\\text{lon}_t),; P(\\text{SOG}_t),; P(\\text{COG}_t)\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ 7. Loss Function\n",
    "\n",
    "The training uses **Cross-Entropy (CE) loss** for each head:\n",
    "[\n",
    "\\mathcal{L}*{fine} = \\text{CE}(P*{lat}, y_{lat}) + \\text{CE}(P_{lon}, y_{lon}) + ...\n",
    "]\n",
    "\n",
    "Plus an optional **coarse loss** term that merges neighboring bins to encourage *smooth* predictions:\n",
    "[\n",
    "\\mathcal{L}*{coarse} = \\text{CE}(P*{lat}^{coarse}, y_{lat}^{coarse}) + ...\n",
    "]\n",
    "Total loss:\n",
    "[\n",
    "\\mathcal{L} = \\mathcal{L}*{fine} + \\beta \\mathcal{L}*{coarse}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ 8. Inference / Generation\n",
    "\n",
    "During prediction:\n",
    "\n",
    "1. Start with the **past sequence** and a learnable `<START>` token.\n",
    "2. Predict next (lat, lon, SOG, COG) from the Transformer.\n",
    "3. Feed the prediction back as input for the next step.\n",
    "4. Repeat for L future steps.\n",
    "\n",
    "The **sampling logic** includes:\n",
    "\n",
    "* *Continuity penalty* ‚Üí discourages big jumps.\n",
    "* *Water mask* ‚Üí prevents going on land.\n",
    "* *Heading prior* ‚Üí prefers motion along COG.\n",
    "* *Anti-zigzag term* ‚Üí smooths turns.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 9. Analogy Summary\n",
    "\n",
    "| Stage                   | Analogy                     | Purpose                                                     |\n",
    "| ----------------------- | --------------------------- | ----------------------------------------------------------- |\n",
    "| **Embedding layers**    | CNN convolution filters     | Encode local semantics of lat/lon/SOG/COG                   |\n",
    "| **Transformer decoder** | RNN with attention          | Model temporal dependencies and motion dynamics             |\n",
    "| **Feedforward heads**   | Fully Connected Classifiers | Map features ‚Üí predicted bins (classification per variable) |\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è 10. Overall Architecture Diagram (Conceptually)\n",
    "\n",
    "```\n",
    "(lat, lon, SOG, COG)\n",
    "       ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇEmbedding‚îÇ  ‚Üê CNN-like feature extraction\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "  Linear Projection + PosEnc\n",
    "       ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Transformer  ‚îÇ  ‚Üê Temporal modeling\n",
    "  ‚îÇ Decoder Stack‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ    Lat Head   Lon Head   SOG Head   COG Head  (FFNN layers)  ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "  Predicted (lat, lon, sog, cog) bins ‚Üí converted to coordinates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **add a clear diagram** (with arrows and components labeled) to visually show this CNN ‚Üí Transformer ‚Üí FFNN pipeline?\n",
    "I can generate it for you as an image or in a one-page PDF summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6a9d9",
   "metadata": {},
   "source": [
    "# TrAISformer ‚Äî A Full Theoretical & Architectural Explanation\n",
    "\n",
    "> **Pipeline:** Embedding (CNN-like) ‚Üí Transformer Decoder (Temporal Model) ‚Üí FFNN Heads (Lat/Lon/SOG/COG)\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Figure Index (place your images here)\n",
    "\n",
    "<!-- Overall architecture block diagram -->\n",
    "![TrAISformer architecture](notebooks\\TrAISformer.png)\n",
    "*Figure 1 ‚Äî End-to-end pipeline: discretized AIS ‚Üí embeddings ‚Üí transformer ‚Üí four heads ‚Üí joint sampler.*\n",
    "\n",
    "<!-- Multi-head attention schematic (queries/keys/values) -->\n",
    "![Multi-head self-attention](docs/img/attention.png)\n",
    "*Figure 2 ‚Äî Multi-head attention and residual path inside one decoder layer.*\n",
    "\n",
    "<!-- Autoregressive generation loop -->\n",
    "![Autoregressive generation](docs/img/generation_loop.png)\n",
    "*Figure 3 ‚Äî Autoregressive decoding with start token, teacher forcing (train), sampling (eval).*\n",
    "\n",
    "<!-- Joint sampler scoring heatmap example (lat√ólon grid with penalties) -->\n",
    "![Joint sampler scoring map](docs/img/joint_sampler.png)\n",
    "*Figure 4 ‚Äî Joint (lat,lon) score map: logits ‚äï continuity ‚äï heading prior ‚äï anti-zigzag ‚äï hard coastline mask.*\n",
    "\n",
    "<!-- Optional: a real trajectory qualitative example (before/after sampler knobs) -->\n",
    "![Qualitative trajectory](docs/img/qualitative_example.png)\n",
    "*Figure 5 ‚Äî Example prediction vs. ground truth with different sampler knob settings.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Problem Setup\n",
    "\n",
    "We observe a length-`T` past AIS sequence and predict the next `L` steps:\n",
    "$$\n",
    "\\mathcal{D} = \\{(\\mathbf{x}_{1:T},\\ \\mathbf{y}_{1:L})\\},\\quad \n",
    "\\mathbf{x}_t=(\\text{lat}_t,\\text{lon}_t,\\text{SOG}_t,\\text{COG}_t).\n",
    "$$\n",
    "\n",
    "All four continuous attributes are **discretized** into uniform bins:\n",
    "$$\n",
    "\\text{lat}_t \\in \\{0,\\dots,n_{\\text{lat}}-1\\},\\ \n",
    "\\text{lon}_t \\in \\{0,\\dots,n_{\\text{lon}}-1\\},\\\n",
    "\\text{SOG}_t \\in \\{0,\\dots,n_{\\text{SOG}}-1\\},\\\n",
    "\\text{COG}_t \\in \\{0,\\dots,n_{\\text{COG}}-1\\}.\n",
    "$$\n",
    "\n",
    "Let the bin specification be:\n",
    "$$\n",
    "\\text{BinSpec} = (\\text{lat}_{\\min},\\text{lat}_{\\max},\\text{lon}_{\\min},\\text{lon}_{\\max},\\text{sog}_{\\max}, n_{\\text{lat}}, n_{\\text{lon}}, n_{\\text{SOG}}, n_{\\text{COG}}).\n",
    "$$\n",
    "\n",
    "Each bin index can be mapped back to **midpoints**; e.g. for latitude:\n",
    "$$\n",
    "\\widehat{\\text{lat}}(i) = \\text{lat}_{\\min} + \\Big(i+\\tfrac{1}{2}\\Big)\\frac{\\text{lat}_{\\max}-\\text{lat}_{\\min}}{n_{\\text{lat}}}\\quad\\text{(and similarly for lon/SOG/COG).}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Embedding Layer (CNN-like feature extraction)\n",
    "\n",
    "For each attribute we learn an embedding table:\n",
    "$$\n",
    "E_{\\text{lat}}\\in\\mathbb{R}^{n_{\\text{lat}}\\times d_{\\text{lat}}},\\quad\n",
    "E_{\\text{lon}}\\in\\mathbb{R}^{n_{\\text{lon}}\\times d_{\\text{lon}}},\\quad\n",
    "E_{\\text{SOG}}\\in\\mathbb{R}^{n_{\\text{SOG}}\\times d_{\\text{SOG}}},\\quad\n",
    "E_{\\text{COG}}\\in\\mathbb{R}^{n_{\\text{COG}}\\times d_{\\text{COG}}}.\n",
    "$$\n",
    "\n",
    "At each time step $t$:\n",
    "$$\n",
    "\\mathbf{e}^{\\text{lat}}_t = E_{\\text{lat}}[\\text{lat}_t],\\ \n",
    "\\mathbf{e}^{\\text{lon}}_t = E_{\\text{lon}}[\\text{lon}_t],\\ \n",
    "\\mathbf{e}^{\\text{SOG}}_t = E_{\\text{SOG}}[\\text{SOG}_t],\\ \n",
    "\\mathbf{e}^{\\text{COG}}_t = E_{\\text{COG}}[\\text{COG}_t].\n",
    "$$\n",
    "\n",
    "Concatenate:\n",
    "$$\n",
    "\\mathbf{e}_t = \\big[\\mathbf{e}^{\\text{lat}}_t \\,\\Vert\\, \\mathbf{e}^{\\text{lon}}_t \\,\\Vert\\, \\mathbf{e}^{\\text{SOG}}_t \\,\\Vert\\, \\mathbf{e}^{\\text{COG}}_t\\big] \\in \\mathbb{R}^{d_{\\text{in}}}.\n",
    "$$\n",
    "\n",
    "**Intuition:** like a CNN builds local features from pixels, these embeddings learn a continuous space where nearby bins (e.g., adjacent grid cells or headings) are close.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Linear Projection + Positional Encoding\n",
    "\n",
    "Project to the model dimension $d$:\n",
    "$$\n",
    "\\mathbf{z}_t = W_{\\text{in}}\\mathbf{e}_t + \\mathbf{b}_{\\text{in}},\\qquad W_{\\text{in}}\\in\\mathbb{R}^{d\\times d_{\\text{in}}}.\n",
    "$$\n",
    "\n",
    "Add **sinusoidal** positional encoding $ \\mathbf{p}_t\\in\\mathbb{R}^d $ (fixed, not learned):\n",
    "$$\n",
    "\\mathbf{h}^{(0)}_t = \\mathbf{z}_t + \\mathbf{p}_t,\\quad\n",
    "p_t^{(2i)} = \\sin\\!\\Big(\\tfrac{t}{10000^{2i/d}}\\Big),\\ \n",
    "p_t^{(2i+1)} = \\cos\\!\\Big(\\tfrac{t}{10000^{2i/d}}\\Big).\n",
    "$$\n",
    "\n",
    "This gives the model a notion of **order in time**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Transformer Decoder Stack (Temporal Modeling)\n",
    "\n",
    "We use $L_{\\text{dec}}$ layers of a **causal** Transformer decoder. For layer $\\ell=1\\ldots L_{\\text{dec}}$ and each time $t$:\n",
    "\n",
    "### 4.1) Masked Self-Attention (over the decoder stream)\n",
    "Queries/Keys/Values are linear projections of $\\mathbf{h}^{(\\ell-1)}_{\\le t}$.\n",
    "$$\n",
    "\\text{Attn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\text{softmax}\\!\\Big(\\tfrac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}} + \\mathbf{M}\\Big)\\mathbf{V},\n",
    "$$\n",
    "where $\\mathbf{M}$ is an upper-triangular mask (‚àí‚àû above the diagonal) enforcing causality.\n",
    "\n",
    "Multi-head attention splits $d$ into $H$ heads; outputs are concatenated and projected.\n",
    "\n",
    "### 4.2) (Optional) Cross-Attention to Past ‚ÄúMemory‚Äù\n",
    "During training/inference we pass the encoded **past** sequence as `memory`.\n",
    "The decoder token at step $t$ can attend over the **past encodings** $\\mathbf{m}_{1:T}$ to exploit long-range context.\n",
    "\n",
    "### 4.3) Position-wise FFN\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}) = \\sigma(\\mathbf{x}W_1+\\mathbf{b}_1)W_2+\\mathbf{b}_2\n",
    "$$\n",
    "with $\\sigma=\\text{GELU/ReLU}$, applied independently at each time step.\n",
    "\n",
    "Each sub-block uses **residual connections** and **LayerNorm**:\n",
    "$$\n",
    "\\mathbf{u}=\\text{LN}\\big(\\mathbf{x}+\\text{SelfAttn}(\\mathbf{x})\\big),\\quad\n",
    "\\mathbf{v}=\\text{LN}\\big(\\mathbf{u}+\\text{CrossAttn}(\\mathbf{u},\\mathbf{m})\\big),\\quad\n",
    "\\mathbf{h}^{(\\ell)}=\\text{LN}\\big(\\mathbf{v}+\\text{FFN}(\\mathbf{v})\\big).\n",
    "$$\n",
    "\n",
    "After the stack, the decoder state at the last position is $\\mathbf{h}_t\\in\\mathbb{R}^{d}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Feed-Forward Output Heads (Four Classifiers)\n",
    "\n",
    "For every predicted step $t$, we map $\\mathbf{h}_t$ to **four** categorical distributions:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}^{\\text{lat}}_t &= \\text{softmax}(\\mathbf{h}_t W_{\\text{lat}} + \\mathbf{b}_{\\text{lat}}) \\in \\Delta^{n_{\\text{lat}}-1},\\\\\n",
    "\\mathbf{p}^{\\text{lon}}_t &= \\text{softmax}(\\mathbf{h}_t W_{\\text{lon}} + \\mathbf{b}_{\\text{lon}}) \\in \\Delta^{n_{\\text{lon}}-1},\\\\\n",
    "\\mathbf{p}^{\\text{SOG}}_t &= \\text{softmax}(\\mathbf{h}_t W_{\\text{SOG}} + \\mathbf{b}_{\\text{SOG}}) \\in \\Delta^{n_{\\text{SOG}}-1},\\\\\n",
    "\\mathbf{p}^{\\text{COG}}_t &= \\text{softmax}(\\mathbf{h}_t W_{\\text{COG}} + \\mathbf{b}_{\\text{COG}}) \\in \\Delta^{n_{\\text{COG}}-1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These are **independent heads** trained with cross-entropy.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Training Objectives (Fine + Coarse)\n",
    "\n",
    "Let $y^{\\text{attr}}_t$ be the ground-truth bin for attribute `attr` at step $t$.\n",
    "The **fine** cross-entropy is:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{fine}}=\\sum_{t=1}^{L}\\sum_{\\text{attr}\\in\\{\\text{lat},\\text{lon},\\text{SOG},\\text{COG}\\}}\n",
    "\\text{CE}\\big(\\mathbf{p}^{\\text{attr}}_t,\\ y^{\\text{attr}}_t\\big).\n",
    "$$\n",
    "\n",
    "To encourage **spatial smoothness**, we add a **coarse** CE on merged bins (merge factor $m$):\n",
    "$$\n",
    "\\mathcal{L}_{\\text{coarse}}=\\sum_{t,\\text{attr}}\\text{CE}\\big(\\text{Merge}_m(\\mathbf{p}^{\\text{attr}}_t),\\ \\text{Merge}_m(y^{\\text{attr}}_t)\\big),\n",
    "$$\n",
    "and optimize\n",
    "$$\n",
    "\\mathcal{L}=\\mathcal{L}_{\\text{fine}}+\\beta\\,\\mathcal{L}_{\\text{coarse}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Autoregressive Generation (Eval)\n",
    "\n",
    "At eval time we use a learnable `<START>` token and roll out for $L$ steps:\n",
    "\n",
    "1. Encode the **past** into memory ($\\mathbf{m}_{1:T}$).\n",
    "2. Decode one step to get $\\mathbf{p}^{\\text{lat}},\\mathbf{p}^{\\text{lon}},\\mathbf{p}^{\\text{SOG}},\\mathbf{p}^{\\text{COG}}$.\n",
    "3. **Sample/argmax** bins; append to outputs.\n",
    "4. Re-embed the sampled bins and feed them to the next step.\n",
    "\n",
    "### Joint (lat,lon) Sampler\n",
    "\n",
    "We build a **2D score** over the (lat,lon) grid by combining:\n",
    "- **Base logits**: outer-sum of lat/lon logits,\n",
    "- **Continuity** (L1 distance to previous cell $(i_0,j_0)$),\n",
    "- **Heading prior** (aim near $(i_c,j_c)$ inferred from SOG/COG),\n",
    "- **Anti-zigzag** (discourage sharp reversals),\n",
    "- **Hard coastline mask** (forbid land).\n",
    "\n",
    "Let $S(i,j)$ be the final score for grid cell $(i,j)$:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "S(i,j) &= \\underbrace{\\ell_{\\text{lat}}(i) + \\ell_{\\text{lon}}(j)}_{\\text{base logits}}\n",
    "\\;-\\; \\lambda_{\\text{cont}}\\,\\bigl(|i-i_0| + |j-j_0|\\bigr) \\\\[2pt]\n",
    "&\\quad-\\; \\alpha_{\\text{dir}}\\bigl(|i-i_c| + |j-j_c|\\bigr) \\\\[2pt]\n",
    "&\\quad-\\; \\beta_{\\text{turn}}\\Bigl(1 - \\cos\\!\\bigl(\\angle(\\Delta\\mathbf{p}_{t-1},\\, (i-i_0,\\ j-j_0))\\bigr)\\Bigr) \\\\[2pt]\n",
    "&\\quad+\\; \\log \\mathbb{1}\\!\\left\\{\\mathrm{water}(i,j)\\right\\} \\quad (\\text{‚àí‚àû on land})\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $\\Delta\\mathbf{p}_{t-1}$ is the last move vector,\n",
    "- $(i_c,j_c)$ is a **small forward aim** from COG/SOG (scaled by grid size via `STEP_SCALE`),\n",
    "- $\\lambda_{\\text{cont}},\\alpha_{\\text{dir}},\\beta_{\\text{turn}}$ are the sampler **knobs**.\n",
    "\n",
    "We then sample from $\\text{Categorical}(\\text{softmax}(S))$ (or take argmax).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Sampler Knobs (the ones in your file)\n",
    "\n",
    "Recommended **global** sampler hyper-parameters (from your implementation):\n",
    "\n",
    "- `LAMBDA_CONT = 0.08` ‚Äî continuity; higher = stickier (less exploration).\n",
    "- `ALPHA_DIR   = 1.00` ‚Äî heading prior strength from (COG,SOG).\n",
    "- `BETA_TURN   = 0.25` ‚Äî anti-zigzag; higher = smoother turns.\n",
    "- `STEP_SCALE  = 0.60` ‚Äî how far the heading ‚Äúaims‚Äù ahead (0.5‚Äì0.8 good).\n",
    "\n",
    "> If the model ‚Äústops‚Äù near the Danish‚ÄìSwedish channel, **increase** `ALPHA_DIR` slightly (e.g., 1.2‚Äì1.6) and **decrease** `BETA_TURN` a bit (e.g., 0.15‚Äì0.20) to allow a decisive turn into the channel. You can also **lower** `LAMBDA_CONT` (e.g., 0.05) to be less sticky near branching choices.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Mathematical Summary (one-screen view)\n",
    "\n",
    "- **Embeddings:** $\\mathbf{e}_t = \\bigoplus_{a\\in\\{\\text{lat},\\text{lon},\\text{SOG},\\text{COG}\\}} E_a[x^a_t]$.\n",
    "- **Projection + PosEnc:** $\\mathbf{h}^{(0)}_t = W_{\\text{in}}\\mathbf{e}_t + \\mathbf{p}_t$.\n",
    "- **Attention:** $\\text{softmax}(\\tfrac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}})\\mathbf{V}$ (masked).\n",
    "- **FFN:** $\\text{FFN}(\\mathbf{x})=\\sigma(\\mathbf{x}W_1)W_2$.\n",
    "- **Heads:** $\\mathbf{p}^{a}_t=\\text{softmax}(\\mathbf{h}_t W_a)$ for $a\\in\\{\\text{lat},\\text{lon},\\text{SOG},\\text{COG}\\}$.\n",
    "- **Loss:** $\\mathcal{L}=\\mathcal{L}_{\\text{fine}}+\\beta\\mathcal{L}_{\\text{coarse}}$.\n",
    "- **Sampler score:** see boxed equation in ¬ß7.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Pseudocode (Train vs. Eval)\n",
    "\n",
    "```python\n",
    "# TRAIN (teacher forcing)\n",
    "for batch in loader:\n",
    "    e_past   = embed(x_past)                 # [B,T,emb_sum]\n",
    "    h_mem    = posenc(proj(e_past))          # memory for cross-attn\n",
    "    e_future = embed(y_future_bins)          # [B,L,emb_sum]\n",
    "    dec_in   = shift_right(proj(e_future))   # prepend <START>\n",
    "    h_dec    = transformer(dec_in, memory=h_mem, causal_mask=True)\n",
    "    logits   = {attr: head[attr](h_dec) for attr in ATTRS}\n",
    "    loss     = fine_ce(logits, y_bins) + beta*coarse_ce(logits, y_bins)\n",
    "    loss.backward(); optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4f6e0",
   "metadata": {},
   "source": [
    "Perfect ‚Äî you‚Äôve now got both **TPTrans** and **TrAISformer** set up and working with your new **water mask**, so let‚Äôs unpack everything in plain terms.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ 1. What each model is doing\n",
    "\n",
    "### **TPTrans (Trajectory Predictor Transformer hybrid)**\n",
    "\n",
    "* **Purpose:** Predict the next *N* ship positions (lat, lon) from the past motion (lat, lon, speed, heading).\n",
    "\n",
    "* **Architecture (from `tptrans.py`):**\n",
    "\n",
    "  1. A **1D CNN** encodes local movement patterns (short-term turns, accelerations).\n",
    "  2. A **Transformer Encoder** captures longer-term dependencies in the sequence.\n",
    "  3. A **GRU Decoder** outputs the next *horizon* points (usually 12 steps ahead).\n",
    "  4. A **Linear layer** projects to predicted latitude/longitude (Œîlat, Œîlon or absolute normalized lat/lon).\n",
    "\n",
    "* **Output:** A continuous trajectory (sequence of predicted positions).\n",
    "  These positions are then denormalized into degrees and plotted in red.\n",
    "\n",
    "* **Limitation:** TPTrans has **no awareness of land/water boundaries** ‚Äî it purely learns motion patterns from data.\n",
    "  Without correction, it can predict ‚Äúpaths‚Äù over land because it only sees coordinates as numbers.\n",
    "\n",
    "---\n",
    "\n",
    "### **TrAISformer (Transformer for AIS data)**\n",
    "\n",
    "* **Purpose:** Predict the next *N* steps in a **discrete, grid-based space** using classification rather than regression.\n",
    "\n",
    "* **Architecture (from `TrAISformer.py`):**\n",
    "\n",
    "  1. The geographic region is split into **bins** (like a 2D grid for lat/lon, plus bins for speed and heading).\n",
    "  2. Each feature (lat, lon, sog, cog) is converted into a **one-hot vector** ‚Üí embedded ‚Üí concatenated.\n",
    "  3. A **causal Transformer decoder** predicts the *next bin* for each feature.\n",
    "  4. A **multi-head output** classifies lat/lon/sog/cog independently at every step.\n",
    "  5. During inference, it can **sample** from probability distributions (stochastic future prediction).\n",
    "\n",
    "* **Output:** A *probabilistic, discrete trajectory* that is later converted back to latitude/longitude degrees.\n",
    "\n",
    "* **Strength:** Because TrAISformer operates in a binned, map-aware way, it integrates the **water mask internally**.\n",
    "  It knows where land is (because during training and generation it avoids invalid land bins).\n",
    "\n",
    "---\n",
    "\n",
    "## üåä 2. The Water Mask ‚Äî what it is and what the numbers mean\n",
    "\n",
    "From your log:\n",
    "\n",
    "```\n",
    "water mask: built (256, 512)  water_fraction=0.546\n",
    "water mask: shape (256, 512)   water_fraction= 0.5463180541992188\n",
    "```\n",
    "\n",
    "This comes from your file [`build_water_mask.py`][33].\n",
    "Here‚Äôs what those numbers mean:\n",
    "\n",
    "| Item                   | Meaning                                                                                                                                       |\n",
    "| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `(256, 512)`           | The mask is a grid with **256 latitude bins** and **512 longitude bins** covering your region of interest (e.g. Denmark or surrounding seas). |\n",
    "| `True` cells           | Represent **water** ‚Äî areas where ships can sail.                                                                                             |\n",
    "| `False` cells          | Represent **land** ‚Äî areas to be avoided.                                                                                                     |\n",
    "| `water_fraction=0.546` | About **54.6% of the grid cells are water**, 45.4% are land. This is realistic for the Danish region.                                         |\n",
    "\n",
    "So the line\n",
    "\n",
    "```\n",
    "water mask: built (256, 512)  water_fraction=0.546\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "> ‚ÄúI built a 256√ó512 map of the area, and roughly half of it is water.‚Äù\n",
    "\n",
    "Then TrAISformer, when initialized, prints:\n",
    "\n",
    "```\n",
    "water mask: shape (256, 512)   water_fraction= 0.5463180541992188\n",
    "```\n",
    "\n",
    "showing it successfully loaded that same grid into its model buffer (`model.water_mask`).\n",
    "\n",
    "---\n",
    "\n",
    "## üö¢ 3. How the Water Mask is Used\n",
    "\n",
    "### a. **In TrAISformer**\n",
    "\n",
    "* The mask (`self.water_mask`) is a boolean grid aligned with the bin structure (`BinSpec`).\n",
    "* During prediction (generation), when the model proposes a new position bin (lat_idx, lon_idx),\n",
    "  if that bin corresponds to **land**, it snaps it to the **nearest water cell** using a fast BFS (breadth-first search).\n",
    "* This prevents land crossings *during generation*.\n",
    "\n",
    "### b. **In TPTrans**\n",
    "\n",
    "* TPTrans doesn‚Äôt have a built-in mask, so in `eval_traj_V4.py` we added:\n",
    "\n",
    "  ```python\n",
    "  ii, jj = to_idx(pred_lat, pred_lon)\n",
    "  ii2, jj2 = snap_to_water_path(ii.copy(), jj.copy(), wm)\n",
    "  pred_lat, pred_lon = idx_to_mid(ii2, jj2)\n",
    "  ```\n",
    "\n",
    "  meaning:\n",
    "\n",
    "  1. Convert predicted coordinates to grid indices.\n",
    "  2. If any predicted points fall on land, BFS to the nearest water pixel.\n",
    "  3. Convert indices back to latitude/longitude.\n",
    "* That ‚Äúsnaps‚Äù any on-land points to the nearest navigable water cell.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 4. Why You See Two Lines About the Mask\n",
    "\n",
    "* `water mask: built (256, 512)...` ‚Üí created by your evaluation script before running predictions (a shared `args._wm` mask).\n",
    "* `water mask: shape (256, 512)...` ‚Üí printed by TrAISformer itself when loading its internal copy.\n",
    "\n",
    "They‚Äôre the same mask printed twice from different parts of the code.\n",
    "If you run TPTrans, you‚Äôll only see the first line (because TPTrans doesn‚Äôt hold its own `model.water_mask`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Summary\n",
    "\n",
    "| Concept             | TrAISformer                                                | TPTrans                                              |\n",
    "| ------------------- | ---------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Input type**      | Discrete bins (grid indices)                               | Continuous normalized values                         |\n",
    "| **Architecture**    | Transformer decoder (auto-regressive)                      | CNN + Transformer encoder + GRU decoder              |\n",
    "| **Output**          | Probability distributions over bins                        | Continuous lat/lon coordinates                       |\n",
    "| **Water awareness** | Built-in: avoids land during generation                    | Added post-hoc: snapped predictions back to water    |\n",
    "| **Best suited for** | Global, categorical modeling (big regions, varied traffic) | Short-term local forecasting (high-frequency motion) |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you a **visual overlay** that explains how the water mask grid aligns with your map (so you can see what the 256√ó512 mask looks like and where the ‚Äúland‚Äù pixels are)? It helps make the concept concrete.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
